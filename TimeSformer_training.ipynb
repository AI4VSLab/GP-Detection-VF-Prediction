{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 3871 patients, 7428 eyes, and 28943 HVFs\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader \n",
    "#from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timesformer_pytorch import TimeSformer  \n",
    "# Load data\n",
    "\n",
    "with open(\"/home/yetian/glaucoma_progression/uwhvf/alldata.json\") as fin:\n",
    "  dat = json.loads(fin.read())\n",
    "\n",
    "# Basic statistics\n",
    "\n",
    "print(f\"Total of {dat['pts']} patients, {dat['eyes']} eyes, and {dat['hvfs']} HVFs\")\n",
    "# Expected output: Total of 3871 patients, 7428 eyes, and 28943 HVFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = [] #7248, 3, 9, 9\n",
    "labellist = [] #7428, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = []\n",
    "for key in dat['data'].keys():\n",
    "    key_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hundred_to_zero(temp2):\n",
    "    for i in range(temp2.shape[0]):\n",
    "        for j in range(temp2.shape[1]):\n",
    "            if temp2[i][j] == 100:\n",
    "                temp2[i][j] = float(0)\n",
    "    return temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate(temp):\n",
    "    a = temp[np.newaxis, :]\n",
    "    a = np.repeat(a, 3, axis=0)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = [] #7248, 3, 9, 9\n",
    "labellist = [] #7428, 1\n",
    "for key in key_list:\n",
    "    if 'L' in dat['data'][key].keys():  #['L'] or ['R']\n",
    "        if len(dat['data'][key]['L']) > 3:  # at least 4 frames\n",
    "            temp0 = np.array(dat['data'][key]['L'][0]['td'])\n",
    "            temp1 = np.array(dat['data'][key]['L'][1]['td'])\n",
    "            temp2 = np.array(dat['data'][key]['L'][2]['td'])\n",
    "\n",
    "            newtemp0 = np.pad(temp0, pad_width=((0,1),(0,0)), mode='constant')   #9,9\n",
    "            newtemp0 = hundred_to_zero(newtemp0)\n",
    "            newtemp0 = duplicate(newtemp0)                                       # 3,9,9\n",
    "            newtemp0 = newtemp0[np.newaxis,:]\n",
    "\n",
    "            newtemp1 = np.pad(temp1, pad_width=((0,1),(0,0)), mode='constant')\n",
    "            newtemp1 = hundred_to_zero(newtemp1)\n",
    "            newtemp1 = duplicate(newtemp1)\n",
    "            newtemp1 = newtemp1[np.newaxis,:]\n",
    "\n",
    "            newtemp2 = np.pad(temp2, pad_width=((0,1),(0,0)), mode='constant')\n",
    "            newtemp2 = hundred_to_zero(newtemp2)\n",
    "            newtemp2 = duplicate(newtemp2)\n",
    "            newtemp2 = newtemp2[np.newaxis,:]\n",
    "\n",
    "            video = np.vstack((newtemp0, newtemp1, newtemp2))                    # 3, 3, 9, 9  #frame, channel, H, W\n",
    "\n",
    "            datalist.append(video)\n",
    "\n",
    "            total_num_of_frames = len(dat['data'][key]['L'])\n",
    "            MD_diff_list = []\n",
    "            for i in range(0, total_num_of_frames - 1):\n",
    "                age1 = dat['data'][key]['L'][i]['age']\n",
    "                age2 = dat['data'][key]['L'][i+1]['age']\n",
    "                temp1 = np.array(dat['data'][key]['L'][i]['td'])\n",
    "                temp1 = hundred_to_zero(temp1)\n",
    "                \n",
    "                temp2 = np.array(dat['data'][key]['L'][i+1]['td'])\n",
    "                temp2 = hundred_to_zero(temp2)\n",
    "                \n",
    "                MD1 = sum(sum(temp1))/np.count_nonzero(temp1)\n",
    "                MD2 = sum(sum(temp2))/np.count_nonzero(temp2)\n",
    "                MD_diff_per_year = (MD2-MD1)/(age2-age1)\n",
    "              #  db = 10 * np.log10(abs(MD1 - MD2))\n",
    "              #  db_per_year = db/abs(age1-age2)\n",
    "                MD_diff_list.append(MD_diff_per_year)\n",
    "            labellist.append(MD_diff_list)\n",
    "\n",
    "\n",
    "for key in key_list:\n",
    "    if 'R' in dat['data'][key].keys():  #['L'] or ['R']\n",
    "        if len(dat['data'][key]['R']) > 3:  # at least 4 frames\n",
    "            temp0 = np.array(dat['data'][key]['R'][0]['td'])\n",
    "            temp1 = np.array(dat['data'][key]['R'][1]['td'])\n",
    "            temp2 = np.array(dat['data'][key]['R'][2]['td'])\n",
    "\n",
    "            newtemp0 = np.pad(temp0, pad_width=((0,1),(0,0)), mode='constant')   #9,9\n",
    "            newtemp0 = hundred_to_zero(newtemp0)\n",
    "            newtemp0 = duplicate(newtemp0)                                       # 3,9,9\n",
    "            newtemp0 = newtemp0[np.newaxis,:]\n",
    "\n",
    "            newtemp1 = np.pad(temp1, pad_width=((0,1),(0,0)), mode='constant')\n",
    "            newtemp1 = hundred_to_zero(newtemp1)\n",
    "            newtemp1 = duplicate(newtemp1)\n",
    "            newtemp1 = newtemp1[np.newaxis,:]\n",
    "\n",
    "            newtemp2 = np.pad(temp2, pad_width=((0,1),(0,0)), mode='constant')\n",
    "            newtemp2 = hundred_to_zero(newtemp2)\n",
    "            newtemp2 = duplicate(newtemp2)\n",
    "            newtemp2 = newtemp2[np.newaxis,:]\n",
    "\n",
    "            video = np.vstack((newtemp0, newtemp1, newtemp2))                    # 3, 3, 9, 9  #frame, channel, H, W\n",
    "\n",
    "            datalist.append(video)\n",
    "            \n",
    "            total_num_of_frames = len(dat['data'][key]['R'])\n",
    "            MD_diff_list = []\n",
    "            for i in range(0, total_num_of_frames - 1):\n",
    "                age1 = dat['data'][key]['R'][i]['age']\n",
    "                age2 = dat['data'][key]['R'][i+1]['age']\n",
    "                temp1 = np.array(dat['data'][key]['R'][i]['td'])\n",
    "                temp1 = hundred_to_zero(temp1)\n",
    "                temp2 = np.array(dat['data'][key]['R'][i+1]['td'])\n",
    "                temp2 = hundred_to_zero(temp2)\n",
    "                MD1 = sum(sum(temp1))/np.count_nonzero(temp1)\n",
    "                MD2 = sum(sum(temp2))/np.count_nonzero(temp2)\n",
    "                MD_diff_per_year = (MD2-MD1)/(age2-age1)\n",
    "               # db = 10 * np.log10(abs(MD1 - MD2))\n",
    "               # db_per_year = db/abs(age1-age2)\n",
    "                MD_diff_list.append(MD_diff_per_year)\n",
    "            labellist.append(MD_diff_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2991\n",
      "2991\n"
     ]
    }
   ],
   "source": [
    "print(len(datalist)) # datalist shape = (3,3,9,9)\n",
    "print(len(labellist))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total 7428 eyes\n",
    "\n",
    "\n",
    "over 4 frames: 2065\n",
    "\n",
    "over 3 frames: 2991\n",
    "\n",
    "over 2 frames: 4452"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2991\n",
      "progressing 2200\n",
      "ratio of progressing: 0.736\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for MD_diff in labellist:\n",
    "    if min(MD_diff) < -1:  # 10log(-1.5) : -0.84  <---> wrong?\n",
    "        count +=1\n",
    "\n",
    "print('total',len(labellist))\n",
    "print('progressing', count)\n",
    "print('ratio of progressing:','{:.3f}'.format(count/len(labellist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.007726415094342,\n",
       " -1.7920555879746738,\n",
       " -0.5054338060393485,\n",
       " 0.30318995208146227,\n",
       " -0.29911762885747256,\n",
       " -0.98614218328841,\n",
       " 1.5352219820056094,\n",
       " -1.1572355217284445,\n",
       " -0.6236105572845692,\n",
       " 0.7072336212112875]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labellist[0]  #'647', 11 frames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of MD changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7955493752725599,\n",
       " -0.17475278744884137,\n",
       " -0.07990652966228588,\n",
       " -0.3885928926099877,\n",
       " 0.7233696285832772,\n",
       " -1.765963380435085,\n",
       " 1.2162008294126148,\n",
       " 0.5923301801744322,\n",
       " 0.21244461046655855]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labellist[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of over 3 frame subjects: 2991\n",
      "among them, progressing number is: 2353\n"
     ]
    }
   ],
   "source": [
    "progression_labellist = []\n",
    "\n",
    "for i in range(len(labellist)):\n",
    "    if min(labellist[i]) < -0.84:\n",
    "        progression_labellist.append(int(1))\n",
    "    else:\n",
    "        progression_labellist.append(int(0))\n",
    "\n",
    "#print(progression_labellist)\n",
    "print('total number of over 3 frame subjects:', len(progression_labellist))\n",
    "print('among them, progressing number is:', progression_labellist.count(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TimeSformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the Dataset class\n",
    "class VFDataset(Dataset):\n",
    "    def __init__(self, label, img, transform=None):\n",
    "        self.label = label\n",
    "        self.img = img\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        image = self.img[index]\n",
    "        label = self.label[index]\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "VF_datasets = {\n",
    "    'train': \n",
    "    VFDataset(img = datalist, label = progression_labellist,\n",
    "                transform=None),\n",
    "}\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(VF_datasets['train'], [2393, 598])\n",
    "\n",
    "dataloaders = {\n",
    "    'train':\n",
    "    torch.utils.data.DataLoader(train_set,\n",
    "                                batch_size=5,\n",
    "                                shuffle=True),\n",
    "    'validation':\n",
    "    torch.utils.data.DataLoader(val_set,\n",
    "                                #val_set,\n",
    "                                \n",
    "                                batch_size=5,\n",
    "                                shuffle=False)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TMSFM(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TMSFM, self).__init__()\n",
    "    self.model = TimeSformer(\n",
    "        dim = 512,\n",
    "        image_size = 9,\n",
    "        patch_size = 3,\n",
    "        num_frames = 3,\n",
    "        num_classes = 1,\n",
    "        depth = 12,\n",
    "        heads = 8,\n",
    "        dim_head =  64,\n",
    "        attn_dropout = 0,\n",
    "        ff_dropout = 0\n",
    "    )\n",
    "  #  self.model.Linear = nn.Linear(512, out_features = 1)\n",
    " #   self.fc = nn.Linear(2,1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #self.model = nn.ModuleList()\n",
    "    x = self.model(x)\n",
    "    \n",
    " #   x = self.fc(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMSFM(\n",
      "  (model): TimeSformer(\n",
      "    (to_patch_embedding): Linear(in_features=27, out_features=512, bias=True)\n",
      "    (frame_rot_emb): RotaryEmbedding()\n",
      "    (image_rot_emb): AxialRotaryEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): PreNorm(\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "              (1): GEGLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): PreNorm(\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "              (1): GEGLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): PreNorm(\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "              (1): GEGLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): PreNorm(\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "              (1): GEGLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (4): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): PreNorm(\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "              (1): GEGLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (5): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): PreNorm(\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "              (1): GEGLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (6): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): PreNorm(\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "              (1): GEGLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (7): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): PreNorm(\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "              (1): GEGLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (8): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): PreNorm(\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "              (1): GEGLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (9): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): PreNorm(\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "              (1): GEGLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (10): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): PreNorm(\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "              (1): GEGLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (11): ModuleList(\n",
      "        (0): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): PreNorm(\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): PreNorm(\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "              (1): GEGLU()\n",
      "              (2): Dropout(p=0, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (to_out): Sequential(\n",
      "      (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=512, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = TMSFM() \n",
    "#model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-frame prediction example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1464],\n",
       "        [0.7509],\n",
       "        [0.8525],\n",
       "        [0.8431],\n",
       "        [0.7729]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = torch.randn(5, 3, 3, 9,9)\n",
    "pred = model(video)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(0.7))\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=3):\n",
    "    since = time.time()\n",
    "\n",
    "    best_f1 = 0\n",
    "\n",
    "    train_f1 = []\n",
    "    val_f1 = []\n",
    "    train_accuracy = []\n",
    "    val_accuracy = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            train_outputs = []\n",
    "            train_preds = []\n",
    "            train_trues = []\n",
    "            output_tensors = []\n",
    "\n",
    "\n",
    "\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "\n",
    "                labels = labels.unsqueeze(1)\n",
    "\n",
    "                # inputs = inputs.to(device)\n",
    "                # labels = labels.to(device)\n",
    "\n",
    "               # print(inputs.shape)\n",
    "               # print(labels.shape)\n",
    "\n",
    "                outputs = model(inputs.float())\n",
    "              #  print(outputs.shape)\n",
    "              #  print('outputs:', outputs)\n",
    "              \n",
    "                loss = criterion(outputs, labels.float())\n",
    "              #  print('loss')\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "               # _, preds = torch.max(outputs, 1)\n",
    "                preds = (outputs >= 0)   #not 0.5!!!\n",
    "                #preds_array = np.asarray(preds)\n",
    "                preds_array = preds.cpu().numpy()                #TypeError: can’t convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first\n",
    "                #labels_array = np.asarray(labels.data)\n",
    "                labels_array = labels.data.cpu().numpy()\n",
    "                outputs_array = outputs.detach().cpu().numpy()\n",
    "\n",
    "                train_outputs.extend(outputs_array) #in tensor\n",
    "                train_preds.extend(preds_array)\n",
    "                train_trues.extend(labels_array)\n",
    "                output_tensors.extend(outputs.detach().cpu())\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "\n",
    "\n",
    "                # from torchmetrics.functional import precision_recall\n",
    "                # precision, recall = precision_recall(preds, labels.data, average='macro', num_classes=2)\n",
    "            #    print('pred',preds)\n",
    "            #    print('dt',labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(train_set)\n",
    "            epoch_acc = running_corrects.double() / len(val_set)\n",
    "\n",
    "            \n",
    "\n",
    "            print('pred array 10:', train_preds[0:10])\n",
    "            print('output array 10:', output_tensors[0:10])\n",
    "            print('trues array 10:', train_trues[0:10])\n",
    "\n",
    "            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "            sklearn_accuracy = accuracy_score(train_trues, train_preds) \n",
    "            sklearn_precision = precision_score(train_trues, train_preds, average='macro')\n",
    "            sklearn_recall = recall_score(train_trues, train_preds, average='macro')\n",
    "            sklearn_f1 = f1_score(train_trues, train_preds, average='macro')\n",
    "            \n",
    "            output_tensors = torch.sigmoid(torch.tensor(output_tensors)).numpy()\n",
    "            fpr, tpr, thresholds = roc_curve(train_trues, output_tensors, pos_label=1)\n",
    "            previous_auc = auc(fpr, tpr)\n",
    "            sklearn_auc = roc_auc_score(train_trues, output_tensors)\n",
    "\n",
    "            #print(train_trues[0:10], train_preds[0:10])\n",
    "\n",
    "            print('{} loss: {:.4f}, torch acc: {:.4f}, sklearn acc: {:.4f}, precision:{:.4f}, recall: {:.4f}, f1_score:{:.4f}, auc:{:.4f}, previous auc:{:.4f}'.format(phase,\n",
    "                                                        epoch_loss,\n",
    "                                                        epoch_acc,\n",
    "                                                        sklearn_accuracy,\n",
    "                                                        sklearn_precision,\n",
    "                                                        sklearn_recall,\n",
    "                                                        sklearn_f1,\n",
    "                                                        sklearn_auc,\n",
    "                                                        previous_auc))\n",
    "            if phase == 'train':\n",
    "              train_f1.append(sklearn_f1)\n",
    "              train_accuracy.append(sklearn_accuracy)\n",
    "\n",
    "\n",
    "            if phase == 'validation':\n",
    "              target_names = ['healthy', 'disease']\n",
    "              print(classification_report(train_trues, train_preds, target_names=target_names))\n",
    "\n",
    "              val_f1.append(sklearn_f1)\n",
    "              val_accuracy.append(sklearn_accuracy)\n",
    "\n",
    "            if phase == 'validation' and sklearn_f1 > best_f1:\n",
    "      #          torch.save(model, '/content/drive/MyDrive/research/best_model/best-model-resnet101.pt')\n",
    "      #          torch.save(model.state_dict(), '/content/drive/MyDrive/research/best_model/best-model-parameters-resnet101.pt')\n",
    "                # method I: plt\n",
    "                save_outputs = output_tensors\n",
    "                save_labels = train_trues\n",
    "                save_preds = train_preds\n",
    "\n",
    "\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.title('Receiver Operating Characteristic')\n",
    "                plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % previous_auc)\n",
    "                plt.legend(loc = 'lower right')\n",
    "                plt.plot([0, 1], [0, 1],'r--')\n",
    "                plt.xlim([0, 1])\n",
    "                plt.ylim([0, 1])\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "           \n",
    "                best_f1 = sklearn_f1\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print('best_f1:', best_f1)\n",
    "                print('A new best model saved at epoch {}!'.format(epoch + 1))\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val F1: {:4f}'.format(best_f1))            \n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_f1, train_accuracy, val_f1, val_accuracy, save_outputs, save_labels, save_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = train_model(model, criterion, optimizer, num_epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
