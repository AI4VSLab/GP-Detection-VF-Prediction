{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch==1.12.1+cu116 in /home/yetian/.local/lib/python3.8/site-packages (1.12.1+cu116)\n",
      "Requirement already satisfied: torchvision==0.13.1+cu116 in /home/yetian/.local/lib/python3.8/site-packages (0.13.1+cu116)\n",
      "Requirement already satisfied: torchaudio==0.12.1 in /home/yetian/.local/lib/python3.8/site-packages (0.12.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /home/yetian/.local/lib/python3.8/site-packages (from torch==1.12.1+cu116) (4.4.0)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from torchvision==0.13.1+cu116) (1.17.4)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision==0.13.1+cu116) (2.22.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision==0.13.1+cu116) (7.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.DifViT import ViT as DifViT\n",
    "from diffusion import GaussianDiffusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPT = 'Shifted Patch Tokenization'\n",
    "LSA = Locality Self-Attention\n",
    "FFNT = If add time embedding before FFN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-05 01:11:04: vit.py[121]: Use time embedding before FFN? 0\n"
     ]
    }
   ],
   "source": [
    "model = DifViT(img_size=9, patch_size=3, num_classes=1, dim=96,\n",
    "                mlp_dim_ratio=2, depth=12, heads=12, dim_head=8, channels=3,\n",
    "                stochastic_depth=0, is_SPT=0, is_LSA=0, ffn_time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_model = GaussianDiffusion(\n",
    "        model,\n",
    "        image_size=9,\n",
    "        channels=1,\n",
    "        timesteps=1000,   # number of steps\n",
    "        loss_type='l1'    # L1 or L2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 9, 9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "dummy = torch.randn(2, 3, 9,9) # (batch x channels x frames x height x width)\n",
    "temp = diffusion_model(dummy)\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 3871 patients, 7428 eyes, and 28943 HVFs\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "with open(\"/home/yetian/glaucoma_progression/uwhvf/alldata.json\") as fin:\n",
    "  dat = json.loads(fin.read())\n",
    "\n",
    "# Basic statistics\n",
    "\n",
    "print(f\"Total of {dat['pts']} patients, {dat['eyes']} eyes, and {dat['hvfs']} HVFs\")\n",
    "# Expected output: Total of 3871 patients, 7428 eyes, and 28943 HVFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = [] #7248, 3, 9, 9\n",
    "labellist = [] #7428, 3, 9, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = []\n",
    "for key in dat['data'].keys():\n",
    "    key_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hundred_to_zero(temp2):\n",
    "    for i in range(temp2.shape[0]):\n",
    "        for j in range(temp2.shape[1]):\n",
    "            if temp2[i][j] == 100:\n",
    "                temp2[i][j] = float(0)\n",
    "    return temp2\n",
    "\n",
    "def duplicate(temp):\n",
    "    a = temp[np.newaxis, :]\n",
    "    a = np.repeat(a, 3, axis=0)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of available eyes 2065\n",
      "average 5.411749887718299\n",
      "median 4.6570841889117105\n",
      "number of >5 yr pairs 914\n",
      "ratio of >5 yr pairs 0.44261501210653753\n"
     ]
    }
   ],
   "source": [
    "gap_list = []\n",
    "for key in key_list:\n",
    "    if 'L' in dat['data'][key].keys(): \n",
    "        if len(dat['data'][key]['L']) > 4:\n",
    "            age_diff = dat['data'][key]['L'][4]['age'] - dat['data'][key]['L'][0]['age']\n",
    "            gap_list.append(age_diff)\n",
    "\n",
    "for key in key_list:\n",
    "    if 'R' in dat['data'][key].keys(): \n",
    "        if len(dat['data'][key]['R']) > 4:\n",
    "            age_diff = dat['data'][key]['R'][4]['age'] - dat['data'][key]['R'][0]['age']\n",
    "            gap_list.append(age_diff)\n",
    "\n",
    "print('total number of available eyes', len(gap_list))\n",
    "print('average',sum(gap_list) / len(gap_list))\n",
    "import statistics\n",
    "print('median',statistics.median(gap_list))\n",
    "\n",
    "counter = 0\n",
    "for gap in gap_list:\n",
    "    if gap >= 5:\n",
    "        counter +=1\n",
    "print('number of >5 yr pairs', counter)\n",
    "print('ratio of >5 yr pairs', counter/len(gap_list))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of ages when VF test taken for one subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.22108145106092\n",
      "52.982888432580424\n",
      "53.94113620807666\n"
     ]
    }
   ],
   "source": [
    "print(dat['data']['2630']['L'][0]['age'])\n",
    "print(dat['data']['2630']['L'][1]['age'])\n",
    "print(dat['data']['2630']['L'][2]['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = []\n",
    "labellist = [] \n",
    "for key in key_list:\n",
    "    if 'L' in dat['data'][key].keys():  #['L'] or ['R']\n",
    "        if len(dat['data'][key]['L']) > 2:  # at least 2 frames\n",
    "            temp0 = np.array(dat['data'][key]['L'][0]['td'])\n",
    "            temp1 = np.array(dat['data'][key]['L'][2]['td'])\n",
    "\n",
    "            newtemp0 = np.pad(temp0, pad_width=((0,1),(0,0)), mode='constant')   #9,9\n",
    "            newtemp0 = hundred_to_zero(newtemp0)\n",
    "            #newtemp0 = duplicate(newtemp0)                                       # 3,9,9\n",
    "            newtemp0 = newtemp0[np.newaxis,:]\n",
    "\n",
    "            newtemp1 = np.pad(temp1, pad_width=((0,1),(0,0)), mode='constant')\n",
    "            newtemp1 = hundred_to_zero(newtemp1)\n",
    "            #newtemp1 = duplicate(newtemp1)\n",
    "            newtemp1 = newtemp1[np.newaxis,:]\n",
    "\n",
    "            datalist.append(newtemp0)\n",
    "            labellist.append(newtemp1)\n",
    "\n",
    "for key in key_list:\n",
    "    if 'R' in dat['data'][key].keys():  #['L'] or ['R']\n",
    "        if len(dat['data'][key]['R']) > 2:  # at least 2 frames\n",
    "            temp0 = np.array(dat['data'][key]['R'][0]['td'])\n",
    "            temp1 = np.array(dat['data'][key]['R'][2]['td'])\n",
    "            \n",
    "            newtemp0 = np.pad(temp0, pad_width=((0,1),(0,0)), mode='constant')   #9,9\n",
    "            newtemp0 = hundred_to_zero(newtemp0)\n",
    "            #newtemp0 = duplicate(newtemp0)                                       # 3,9,9\n",
    "            newtemp0 = newtemp0[np.newaxis,:]\n",
    "\n",
    "            newtemp1 = np.pad(temp1, pad_width=((0,1),(0,0)), mode='constant')\n",
    "            newtemp1 = hundred_to_zero(newtemp1)\n",
    "            #newtemp1 = duplicate(newtemp1)\n",
    "            newtemp1 = newtemp1[np.newaxis,:]\n",
    "\n",
    "            datalist.append(newtemp0)\n",
    "            labellist.append(newtemp1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure input and label have same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4452\n",
      "4452\n"
     ]
    }
   ],
   "source": [
    "print(len(datalist)) # datalist shape = (3,3,9,9)\n",
    "print(len(labellist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader \n",
    "#from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# defining the Dataset class\n",
    "class VFDataset(Dataset):\n",
    "    def __init__(self, label, img, transform=None):\n",
    "        self.label = label\n",
    "        self.img = img\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        image = self.img[index]\n",
    "        label = self.label[index]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "VF_datasets = {\n",
    "    'train': \n",
    "    VFDataset(img = datalist, label = labellist,\n",
    "                transform=None),\n",
    "}\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(VF_datasets['train'], [3561,891])\n",
    "\n",
    "dataloaders = {\n",
    "    'train':\n",
    "    torch.utils.data.DataLoader(train_set,\n",
    "                                batch_size=5,\n",
    "                                shuffle=True),\n",
    "    'validation':\n",
    "    torch.utils.data.DataLoader(val_set,\n",
    "                                #val_set,\n",
    "                                \n",
    "                                batch_size=5,\n",
    "                                shuffle=False)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GENVIT(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GENVIT, self).__init__()\n",
    "    self.model = DifViT(img_size=9, patch_size=1, num_classes=1, dim=192,\n",
    "                mlp_dim_ratio=2, depth=12, heads=12, dim_head=16, channels=1,\n",
    "                stochastic_depth=0, is_SPT=0, is_LSA=1, ffn_time=1)\n",
    "\n",
    "    self.diffusion_model = GaussianDiffusion(\n",
    "        self.model,\n",
    "        image_size=9,\n",
    "        channels=1,\n",
    "        timesteps=1000,   # number of steps\n",
    "        loss_type='l1'    # L1 or L2\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    x = self.diffusion_model(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02-05 01:11:08: vit.py[121]: Use time embedding before FFN? 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENVIT(\n",
      "  (model): ViT(\n",
      "    (time_mlp): Sequential(\n",
      "      (0): SinusoidalPosEmb()\n",
      "      (1): Linear(in_features=192, out_features=328, bias=True)\n",
      "      (2): GELU(approximate=none)\n",
      "      (3): Linear(in_features=328, out_features=328, bias=True)\n",
      "    )\n",
      "    (to_patch_embedding): Sequential(\n",
      "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=1, p2=1)\n",
      "      (1): Linear(in_features=1, out_features=192, bias=True)\n",
      "    )\n",
      "    (recon_head): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=1, bias=True)\n",
      "      (1): Rearrange('b (h w) (p1 p2 c) -> b c (h p1) (w p2)', h=9, w=9, p1=1, p2=1)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (transformer): Transformer(\n",
      "      (layers): ModuleList(\n",
      "        (0): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (attend): Softmax(dim=-1)\n",
      "              (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (attend): Softmax(dim=-1)\n",
      "              (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (attend): Softmax(dim=-1)\n",
      "              (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (attend): Softmax(dim=-1)\n",
      "              (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (attend): Softmax(dim=-1)\n",
      "              (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (attend): Softmax(dim=-1)\n",
      "              (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (6): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (attend): Softmax(dim=-1)\n",
      "              (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (7): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (attend): Softmax(dim=-1)\n",
      "              (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (8): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (attend): Softmax(dim=-1)\n",
      "              (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (9): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (attend): Softmax(dim=-1)\n",
      "              (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (10): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (attend): Softmax(dim=-1)\n",
      "              (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (11): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): Attention(\n",
      "              (attend): Softmax(dim=-1)\n",
      "              (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (1): GELU(approximate=none)\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (time_layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "        )\n",
      "        (8): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "        )\n",
      "        (9): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "        )\n",
      "        (10): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "        )\n",
      "        (11): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "    )\n",
      "    (mlp_head): Sequential(\n",
      "      (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=192, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (diffusion_model): GaussianDiffusion(\n",
      "    (denoise_fn): ViT(\n",
      "      (time_mlp): Sequential(\n",
      "        (0): SinusoidalPosEmb()\n",
      "        (1): Linear(in_features=192, out_features=328, bias=True)\n",
      "        (2): GELU(approximate=none)\n",
      "        (3): Linear(in_features=328, out_features=328, bias=True)\n",
      "      )\n",
      "      (to_patch_embedding): Sequential(\n",
      "        (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=1, p2=1)\n",
      "        (1): Linear(in_features=1, out_features=192, bias=True)\n",
      "      )\n",
      "      (recon_head): Sequential(\n",
      "        (0): Linear(in_features=192, out_features=1, bias=True)\n",
      "        (1): Rearrange('b (h w) (p1 p2 c) -> b c (h p1) (w p2)', h=9, w=9, p1=1, p2=1)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (transformer): Transformer(\n",
      "        (layers): ModuleList(\n",
      "          (0): ModuleList(\n",
      "            (0): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): Attention(\n",
      "                (attend): Softmax(dim=-1)\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate=none)\n",
      "                  (2): Dropout(p=0.0, inplace=False)\n",
      "                  (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                  (4): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): ModuleList(\n",
      "            (0): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): Attention(\n",
      "                (attend): Softmax(dim=-1)\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate=none)\n",
      "                  (2): Dropout(p=0.0, inplace=False)\n",
      "                  (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                  (4): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (2): ModuleList(\n",
      "            (0): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): Attention(\n",
      "                (attend): Softmax(dim=-1)\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate=none)\n",
      "                  (2): Dropout(p=0.0, inplace=False)\n",
      "                  (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                  (4): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (3): ModuleList(\n",
      "            (0): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): Attention(\n",
      "                (attend): Softmax(dim=-1)\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate=none)\n",
      "                  (2): Dropout(p=0.0, inplace=False)\n",
      "                  (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                  (4): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (4): ModuleList(\n",
      "            (0): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): Attention(\n",
      "                (attend): Softmax(dim=-1)\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate=none)\n",
      "                  (2): Dropout(p=0.0, inplace=False)\n",
      "                  (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                  (4): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (5): ModuleList(\n",
      "            (0): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): Attention(\n",
      "                (attend): Softmax(dim=-1)\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate=none)\n",
      "                  (2): Dropout(p=0.0, inplace=False)\n",
      "                  (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                  (4): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (6): ModuleList(\n",
      "            (0): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): Attention(\n",
      "                (attend): Softmax(dim=-1)\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate=none)\n",
      "                  (2): Dropout(p=0.0, inplace=False)\n",
      "                  (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                  (4): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (7): ModuleList(\n",
      "            (0): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): Attention(\n",
      "                (attend): Softmax(dim=-1)\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate=none)\n",
      "                  (2): Dropout(p=0.0, inplace=False)\n",
      "                  (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                  (4): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (8): ModuleList(\n",
      "            (0): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): Attention(\n",
      "                (attend): Softmax(dim=-1)\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate=none)\n",
      "                  (2): Dropout(p=0.0, inplace=False)\n",
      "                  (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                  (4): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (9): ModuleList(\n",
      "            (0): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): Attention(\n",
      "                (attend): Softmax(dim=-1)\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate=none)\n",
      "                  (2): Dropout(p=0.0, inplace=False)\n",
      "                  (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                  (4): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (10): ModuleList(\n",
      "            (0): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): Attention(\n",
      "                (attend): Softmax(dim=-1)\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate=none)\n",
      "                  (2): Dropout(p=0.0, inplace=False)\n",
      "                  (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                  (4): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (11): ModuleList(\n",
      "            (0): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): Attention(\n",
      "                (attend): Softmax(dim=-1)\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=384, bias=True)\n",
      "                  (1): GELU(approximate=none)\n",
      "                  (2): Dropout(p=0.0, inplace=False)\n",
      "                  (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "                  (4): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (time_layers): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "          )\n",
      "          (3): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "          )\n",
      "          (4): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "          )\n",
      "          (5): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "          )\n",
      "          (6): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "          )\n",
      "          (7): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "          )\n",
      "          (8): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "          )\n",
      "          (9): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "          )\n",
      "          (10): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "          )\n",
      "          (11): Sequential(\n",
      "            (0): SiLU()\n",
      "            (1): Linear(in_features=328, out_features=164, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (mlp_head): Sequential(\n",
      "        (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=192, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GENVIT() \n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 9, 9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = torch.randn(5, 1, 9,9)\n",
    "pred = model(video)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "MAE = nn.L1Loss()\n",
    "criterion = MAE\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=3):\n",
    "    since = time.time()\n",
    "\n",
    "    best_mse = 10000\n",
    "\n",
    "    train_mse = []\n",
    "    val_mse = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            train_preds = []\n",
    "            train_trues = []\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "\n",
    "               # labels = labels.unsqueeze(1)\n",
    "               # print(inputs.shape)\n",
    "                outputs = model(inputs.float())\n",
    "              #  print(outputs.shape)\n",
    "              #  print('outputs:', outputs)\n",
    "              \n",
    "                loss = criterion(outputs, labels.float())\n",
    "              #  print('loss')\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                preds_array = outputs.detach().cpu().numpy()          \n",
    "                labels_array = labels.data.detach().cpu().numpy()\n",
    "\n",
    "                train_preds.extend(preds_array)\n",
    "                train_trues.extend(labels_array)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if phase == 'train':\n",
    "              epoch_loss = running_loss / len(train_set)\n",
    "            else:\n",
    "              epoch_loss = running_loss / len(val_set)\n",
    "              \n",
    "\n",
    "\n",
    "            print('{} loss: {:.4f}'.format(phase,epoch_loss))\n",
    "\n",
    "            if phase == 'train':\n",
    "              train_mse.append(epoch_loss)\n",
    "\n",
    "            if phase == 'validation':\n",
    "              val_mse.append(epoch_loss)\n",
    "\n",
    "            if phase == 'validation' and epoch_loss < best_mse:\n",
    "                save_labels = train_trues\n",
    "                save_preds = train_preds\n",
    "\n",
    "                best_mse = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print('best_mae:', best_mse)\n",
    "                print()\n",
    "                print('A new best model saved at epoch {}!'.format(epoch + 1))\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val mae: {:4f}'.format(best_mse))            \n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_mse, val_mse, best_mse, save_labels, save_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = train_model(model, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  0.0000,   0.0000,   0.0000, -10.4800,  -6.9000,  -7.6700, -17.3300,\n",
      "             0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,  -8.0200,  -5.9500,  -4.4200,  -8.0500,  -6.6500,\n",
      "            -3.8300,   0.0000],\n",
      "          [  0.0000, -11.6000,  -8.9100,  -5.5200,  -9.2500,  -6.5600,  -5.4000,\n",
      "            -2.7900,  -2.5200],\n",
      "          [ -8.4000,  -4.9200,  -7.0200,  -8.8300,  -7.6000,  -6.2800,  -4.8000,\n",
      "            24.0000,  -5.8100],\n",
      "          [ -3.8000,  -5.7400,  -3.8800,  -4.1100,  -7.8800,  -5.4300,  -2.4900,\n",
      "             0.0000,  -3.3600],\n",
      "          [  0.0000,  -8.0600,  -6.3200,  -3.9400,  -7.4100,  -2.0100,  -3.5000,\n",
      "            -5.7600,  -3.2100],\n",
      "          [  0.0000,   0.0000,  -4.8600,  -6.5800,  -8.1200,  -4.7300,  -2.1100,\n",
      "            -2.3500,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  -9.5600,  -4.7800,  -2.4200,  -2.7500,\n",
      "             0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "             0.0000,   0.0000]]]], dtype=torch.float64)\n",
      "tensor([[[[ 7.9973e-02,  7.1351e-02,  3.0273e-02, -6.9390e+00, -5.0993e+00,\n",
      "           -4.9987e+00, -8.6231e+00,  6.5307e-03,  3.3230e-02],\n",
      "          [ 5.9834e-02,  6.0287e-02, -5.7469e+00, -5.1659e+00, -4.2567e+00,\n",
      "           -5.6474e+00, -5.3161e+00, -3.8305e+00,  3.5617e-02],\n",
      "          [-8.6344e-02, -7.5319e+00, -5.0793e+00, -3.4924e+00, -4.9664e+00,\n",
      "           -4.8517e+00, -4.5280e+00, -3.7504e+00, -3.2460e+00],\n",
      "          [-6.3844e+00, -4.7227e+00, -5.2122e+00, -5.9113e+00, -4.9185e+00,\n",
      "           -4.8425e+00, -4.4268e+00,  2.1625e+01, -3.9480e+00],\n",
      "          [-3.7804e+00, -4.8580e+00, -4.6761e+00, -4.0673e+00, -4.8658e+00,\n",
      "           -3.6350e+00, -2.9627e+00,  1.3924e+00, -3.5211e+00],\n",
      "          [ 1.0177e-01, -6.8331e+00, -5.3538e+00, -3.5714e+00, -5.7634e+00,\n",
      "           -3.3786e+00, -3.5660e+00, -4.7498e+00, -4.1007e+00],\n",
      "          [-1.4908e-01,  4.3220e-01, -4.5446e+00, -5.2323e+00, -5.9417e+00,\n",
      "           -4.9054e+00, -2.6374e+00, -4.3598e+00, -1.0500e-01],\n",
      "          [ 3.6395e-02,  9.9023e-02,  1.2870e-01, -6.7790e+00, -4.8463e+00,\n",
      "           -3.8655e+00, -4.0757e+00,  1.9922e-02,  5.7981e-02],\n",
      "          [ 3.8855e-02,  7.1821e-02, -2.2262e-01,  3.4922e-02, -4.8144e-02,\n",
      "           -7.3325e-02,  1.0330e-01,  1.0987e-01, -3.2528e-01]]]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "MAE tensor(1.0955, grad_fn=<L1LossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "example1 = datalist[1]\n",
    "example1 = example1[np.newaxis, :]\n",
    "example1 = torch.from_numpy(example1)\n",
    "pred1 = model(example1.float())\n",
    "print(example1)\n",
    "print(pred1)\n",
    "print('MAE', MAE(pred1, example1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
